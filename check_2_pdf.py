# ===================== IMPORTS =====================
import os, re, io
from typing import Dict, Any, List
from pathlib import Path

from chromadb.config import Settings
from dotenv import load_dotenv
load_dotenv(override=True)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, AIMessage # Import AIMessage
from langchain_community.document_loaders import PyMuPDFLoader

# ===================== ENV =====================
OPENAI__API_KEY = os.getenv("OPENAI__API_KEY")
OPENAI__EMBEDDING_MODEL = os.getenv("OPENAI__EMBEDDING_MODEL")
OPENAI__MODEL_NAME = os.getenv("OPENAI__MODEL_NAME")
OPENAI__TEMPERATURE = os.getenv("OPENAI__TEMPERATURE")

llm = ChatOpenAI(
    api_key=OPENAI__API_KEY,
    model_name=OPENAI__MODEL_NAME,
    temperature=float(OPENAI__TEMPERATURE) if OPENAI__TEMPERATURE else 0
)

# ===================== VECTORDB =====================
VECTORDB_PATH = r"./vectordb_storage_1"
os.makedirs(VECTORDB_PATH, exist_ok=True)

emb = OpenAIEmbeddings(api_key=OPENAI__API_KEY, model=OPENAI__EMBEDDING_MODEL)

vectordb = None
retriever = None

# ===================== PDF FOLDER =====================
PDF_FOLDER = "./data_1"


def get_pdf_files_from_folder(folder_path: str) -> List[str]:
    """L·∫•y t·∫•t c·∫£ file PDF trong folder"""
    pdf_files = []
    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Folder kh√¥ng t·ªìn t·∫°i: {folder_path}")
        return pdf_files
    
    for file in os.listdir(folder_path):
        if file.lower().endswith('.pdf'):
            full_path = os.path.join(folder_path, file)
            pdf_files.append(full_path)
    
    return sorted(pdf_files)  # Sort ƒë·ªÉ c√≥ th·ª© t·ª± nh·∫•t qu√°n

PDF_PATHS = get_pdf_files_from_folder(PDF_FOLDER)

# ===================== SYSTEM PROMPT =====================
PDF_READER_SYS = (
    "B·∫°n l√† m·ªôt **tr·ª£ l√Ω AI ph√°p l√Ω** chuy√™n ƒë·ªçc hi·ªÉu v√† tra c·ª©u c√°c t√†i li·ªáu PDF ƒë∆∞·ª£c cung c·∫•p "
    "(bao g·ªìm: Lu·∫≠t, Ngh·ªã ƒë·ªãnh, Quy·∫øt ƒë·ªãnh, Th√¥ng t∆∞, VƒÉn b·∫£n h·ª£p nh·∫•t, Quy ho·∫°ch, Danh m·ª•c khu c√¥ng nghi·ªáp, v.v.). "
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† **tr√≠ch xu·∫•t v√† tr·∫£ l·ªùi ch√≠nh x√°c c√°c th√¥ng tin c√≥ trong t√†i li·ªáu**, "
    "ƒë·∫∑c bi·ªát li√™n quan ƒë·∫øn **Lao ƒë·ªông**, **D√¢n s·ª±** v√† **c√°c Khu c√¥ng nghi·ªáp, C·ª•m c√¥ng nghi·ªáp t·∫°i Vi·ªát Nam**.\n\n"

    "‚öôÔ∏è **QUY T·∫ÆC ƒê·∫∂C BI·ªÜT:**\n"
    "- N·∫øu ng∆∞·ªùi d√πng ch·ªâ ch√†o h·ªèi ho·∫∑c ƒë·∫∑t c√¢u h·ªèi chung chung (v√≠ d·ª•: 'xin ch√†o', 'b·∫°n l√†m ƒë∆∞·ª£c g√¨', 'gi√∫p t√¥i v·ªõi' ...), "
    "h√£y tr·∫£ l·ªùi **nguy√™n vƒÉn** nh∆∞ sau:\n"
    "'Xin ch√†o! M√¨nh l√† Chatbot C·ªïng vi·ªác l√†m Vi·ªát Nam. M√¨nh c√≥ th·ªÉ gi√∫p anh/ch·ªã tra c·ª©u v√† gi·∫£i th√≠ch c√°c quy ƒë·ªãnh ph√°p lu·∫≠t "
    "(lu·∫≠t, ngh·ªã ƒë·ªãnh, th√¥ng t∆∞...) li√™n quan ƒë·∫øn lao ƒë·ªông, vi·ªác l√†m, d√¢n s·ª± v√† c√°c lƒ©nh v·ª±c ph√°p l√Ω kh√°c. "
    "G√µ c√¢u h·ªèi c·ª• th·ªÉ ho·∫∑c m√¥ t·∫£ t√¨nh hu·ªëng nh√© ‚Äî m√¨nh s·∫Ω tr·∫£ l·ªùi ng·∫Øn g·ªçn, c√≥ d·∫´n ngu·ªìn.'\n\n"

    "üìò **NGUY√äN T·∫ÆC CHUNG KHI TR·∫¢ L·ªúI:**\n"
    "1) **Ph·∫°m vi:** Ch·ªâ d·ª±a v√†o n·ªôi dung trong c√°c t√†i li·ªáu PDF ƒë√£ ƒë∆∞·ª£c cung c·∫•p; tuy·ªát ƒë·ªëi kh√¥ng s·ª≠ d·ª•ng ho·∫∑c suy di·ªÖn ki·∫øn th·ª©c b√™n ngo√†i.\n"
    "2) **Ngu·ªìn tr√≠ch d·∫´n:** Khi c√≥ th·ªÉ, d·∫´n r√µ ngu·ªìn g·ªëc (v√≠ d·ª•: 'Theo ƒêi·ªÅu X, Ngh·ªã ƒë·ªãnh s·ªë Y/Nƒê-CP...').\n"
    "3) **Ng√¥n ng·ªØ:** S·ª≠ d·ª•ng vƒÉn phong ph√°p l√Ω, trung l·∫≠p, r√µ r√†ng v√† t√¥n tr·ªçng ng·ªØ ƒëi·ªáu h√†nh ch√≠nh.\n"
    "4) **Tr√¨nh b√†y:** ∆Øu ti√™n tr√¨nh b√†y d∆∞·ªõi d·∫°ng danh s√°ch (s·ªë th·ª© t·ª± ho·∫∑c g·∫°ch ƒë·∫ßu d√≤ng) ƒë·ªÉ d·ªÖ theo d√µi.\n"
    "5) **N·∫øu th√¥ng tin kh√¥ng c√≥:** Tr·∫£ l·ªùi r√µ r√†ng: 'Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.'\n"
    "6) **N·∫øu c√¢u h·ªèi m∆° h·ªì:** Y√™u c·∫ßu ng∆∞·ªùi d√πng l√†m r√µ ho·∫∑c b·ªï sung chi ti·∫øt ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.\n\n"

    "üè≠ **QUY ƒê·ªäNH RI√äNG ƒê·ªêI V·ªöI C√ÅC KHU C√îNG NGHI·ªÜP / C·ª§M C√îNG NGHI·ªÜP:**\n"
    "1) N·∫øu ng∆∞·ªùi d√πng h·ªèi **'T·ªânh/th√†nh ph·ªë n√†o c√≥ bao nhi√™u khu ho·∫∑c c·ª•m c√¥ng nghi·ªáp'**, "
    "h√£y tr·∫£ l·ªùi theo **ƒë·ªãnh d·∫°ng sau**:\n"
    " ¬† - S·ªë l∆∞·ª£ng khu/c·ª•m c√¥ng nghi·ªáp trong t·ªânh ho·∫∑c th√†nh ph·ªë ƒë√≥.\n"
    " ¬† - Danh s√°ch t√™n c·ªßa t·∫•t c·∫£ c√°c khu/c·ª•m (ch·ªâ t√™n, kh√¥ng n√™u chi ti·∫øt kh√°c).\n\n"
    " ¬† V√≠ d·ª•:\n"
    " ¬† 'T·ªânh B√¨nh D∆∞∆°ng c√≥ 29 khu c√¥ng nghi·ªáp. Bao g·ªìm:\n"
    " ¬† - Khu c√¥ng nghi·ªáp S√≥ng Th·∫ßn 1\n"
    " ¬† - Khu c√¥ng nghi·ªáp VSIP 1\n"
    " ¬† - Khu c√¥ng nghi·ªáp M·ªπ Ph∆∞·ªõc 3\n"
    " ¬† ...'\n\n"

    "2) N·∫øu ng∆∞·ªùi d√πng h·ªèi **chi ti·∫øt v·ªÅ m·ªôt khu/c·ª•m c√¥ng nghi·ªáp c·ª• th·ªÉ (l·∫ßn ƒë·∫ßu ti√™n)**, h√£y tr√¨nh b√†y ƒë·∫ßy ƒë·ªß th√¥ng tin (n·∫øu c√≥ trong t√†i li·ªáu), g·ªìm:\n"
    " ¬† - T√™n khu c√¥ng nghi·ªáp / c·ª•m c√¥ng nghi·ªáp\n"
    " ¬† - ƒê·ªãa ƒëi·ªÉm (t·ªânh/th√†nh ph·ªë, huy·ªán/th·ªã x√£)\n"
    " ¬† - Di·ªán t√≠ch (ha ho·∫∑c m¬≤)\n"
    " ¬† - C∆° quan qu·∫£n l√Ω / ch·ªß ƒë·∫ßu t∆∞\n"
    " ¬† - Quy·∫øt ƒë·ªãnh th√†nh l·∫≠p ho·∫∑c ph√™ duy·ªát quy ho·∫°ch\n"
    " ¬† - Ng√†nh ngh·ªÅ ho·∫°t ƒë·ªông ch√≠nh\n"
    " ¬† - T√¨nh tr·∫°ng ho·∫°t ƒë·ªông (ƒëang ho·∫°t ƒë·ªông / ƒëang quy ho·∫°ch / ƒëang x√¢y d·ª±ng)\n"
    " ¬† - C√°c th√¥ng tin kh√°c li√™n quan (n·∫øu c√≥)\n\n"

    "3) N·∫øu ng∆∞·ªùi d√πng **ti·∫øp t·ª•c h·ªèi chi ti·∫øt** v·ªÅ c√°c c·ª•m ho·∫∑c khu c√¥ng nghi·ªáp (t·ª´ l·∫ßn th·ª© hai tr·ªü ƒëi), "
    "h√£y **kh√¥ng li·ªát k√™ l·∫°i th√¥ng tin chi ti·∫øt**, m√† **tr·∫£ l·ªùi c·ªë ƒë·ªãnh** nh∆∞ sau:\n"
    "'N·∫øu b·∫°n mu·ªën bi·∫øt th√™m th√¥ng tin chi ti·∫øt v·ªÅ c√°c c·ª•m, h√£y truy c·∫≠p v√†o website https://iipmap.com/.'\n\n"

    "4) N·∫øu ng∆∞·ªùi d√πng ch·ªâ h·ªèi th·ªëng k√™ (v√≠ d·ª•: 'T·ªânh B·∫Øc Ninh c√≥ bao nhi√™u c·ª•m c√¥ng nghi·ªáp?'), "
    "h√£y lu√¥n tr·∫£ l·ªùi s·ªë l∆∞·ª£ng v√† li·ªát k√™ t√™n c·ª•m/khu theo quy ƒë·ªãnh t·∫°i m·ª•c (1) ·ªü tr√™n.\n\n"

    "5) N·∫øu ng∆∞·ªùi d√πng h·ªèi **c√¢u ngo√†i ph·∫°m vi ph√°p lu·∫≠t ho·∫∑c khu/c·ª•m c√¥ng nghi·ªáp** "
    "(v√≠ d·ª•: h·ªèi v·ªÅ tuy·ªÉn d·ª•ng, gi√° ƒë·∫•t, ƒë·∫ßu t∆∞ c√° nh√¢n, v.v.), "
    "h√£y tr·∫£ l·ªùi nguy√™n vƒÉn nh∆∞ sau:\n"
    "'Anh/ch·ªã vui l√≤ng ƒë·ªÉ l·∫°i t√™n v√† s·ªë ƒëi·ªán tho·∫°i, chuy√™n gia c·ªßa IIP s·∫Ω li√™n h·ªá v√† gi·∫£i ƒë√°p c√°c y√™u c·∫ßu c·ªßa anh/ch·ªã ·∫°.'\n\n"
)

# ===================== VECTORDB UTILS =====================
def build_context_from_hits(hits, max_chars: int = 6000) -> str:
    """X√¢y d·ª±ng context t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm"""
    ctx = []
    total = 0
    for idx, h in enumerate(hits, start=1):
        source = h.metadata.get('source', 'unknown')
        seg = f"[{idx}] (Ngu·ªìn: {source})\n{h.page_content.strip()}"
        if total + len(seg) > max_chars:
            break
        ctx.append(seg)
        total += len(seg)
    return "\n\n".join(ctx)

def get_existing_sources() -> set:
    """L·∫•y danh s√°ch file ƒë√£ c√≥ trong VectorDB"""
    global vectordb
    
    if vectordb is None:
        return set()
    
    try:
        collection = vectordb._collection
        existing_data = collection.get()
        
        if existing_data and existing_data.get('metadatas'):
            return set(m.get('source', '') for m in existing_data['metadatas'] if m and m.get('source'))
        
        return set()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi l·∫•y danh s√°ch file: {e}")
        return set()

def check_vectordb_exists() -> bool:
    """Ki·ªÉm tra xem VectorDB c√≥ ƒë·ªß t·∫•t c·∫£ file PDF kh√¥ng"""
    global vectordb
    
    if vectordb is None:
        return False
    
    try:
        collection = vectordb._collection
        count = collection.count()
        
        if count == 0:
            return False
        
        # Ki·ªÉm tra xem ƒë√£ c√≥ ƒë·ªß t·∫•t c·∫£ file PDF ch∆∞a
        target_files = set(os.path.basename(p) for p in PDF_PATHS)
        existing_sources = get_existing_sources()
        
        return target_files.issubset(existing_sources)
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra VectorDB: {e}")
        return False

def get_vectordb_stats() -> Dict[str, Any]:
    """L·∫•y th√¥ng tin th·ªëng k√™ v·ªÅ VectorDB"""
    global vectordb
    
    if vectordb is None:
        return {"total_documents": 0, "path": VECTORDB_PATH, "exists": False}
    
    try:
        collection = vectordb._collection
        count = collection.count()
        
        # L·∫•y danh s√°ch file ƒë√£ n·∫°p
        sources = get_existing_sources()
        
        return {
            "total_documents": count,
            "path": VECTORDB_PATH,
            "exists": count > 0,
            "sources": list(sources)
        }
    except Exception as e:
        return {
            "total_documents": 0,
            "path": VECTORDB_PATH,
            "exists": False,
            "error": str(e)
        }

# ===================== INGEST MULTIPLE PDFs (INCREMENTAL) =====================
def ingest_pdf(pdf_paths=None, vectordb_path=None, emb_fn=None, force_reload=False):
    """
    N·∫°p t√†i li·ªáu PDF v√†o VectorDB (incremental update)
    
    Args:
        pdf_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n file PDF
        vectordb_path: ƒê∆∞·ªùng d·∫´n l∆∞u VectorDB
        emb_fn: H√†m embedding
        force_reload: N·∫øu True, x√≥a VectorDB c≈© v√† n·∫°p l·∫°i to√†n b·ªô
    """
    global vectordb, retriever

    pdf_paths = pdf_paths if pdf_paths is not None else PDF_PATHS
    vectordb_path = vectordb_path if vectordb_path is not None else VECTORDB_PATH
    emb_fn = emb_fn if emb_fn is not None else emb

    print("üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra v√† n·∫°p t√†i li·ªáu PDF...\n")

    # N·∫øu force reload, x√≥a to√†n b·ªô v√† n·∫°p l·∫°i
    if force_reload:
        print("üóëÔ∏è Ch·∫ø ƒë·ªô force reload - X√≥a to√†n b·ªô VectorDB...")
        try:
            temp_db = Chroma(
                collection_name="luat_tong_hop_v1",
                embedding_function=emb_fn,
                persist_directory=vectordb_path,
            )
            temp_db.delete_collection()
            print("‚úÖ ƒê√£ x√≥a VectorDB c≈©\n")
            vectordb = None
        except Exception as e:
            print(f"‚ÑπÔ∏è Kh√¥ng c√≥ VectorDB c≈© ƒë·ªÉ x√≥a: {e}\n")

    # Kh·ªüi t·∫°o ho·∫∑c load VectorDB
    if vectordb is None:
        try:
            vectordb = Chroma(
                collection_name="luat_tong_hop_v1",
                embedding_function=emb_fn,
                persist_directory=vectordb_path,
            )
            print("üìÇ ƒê√£ kh·ªüi t·∫°o/load VectorDB")
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o VectorDB: {e}")
            return None

    # L·∫•y danh s√°ch file ƒë√£ c√≥ trong VectorDB
    existing_sources = get_existing_sources()
    print(f"üìä VectorDB hi·ªán c√≥: {len(existing_sources)} file")
    if existing_sources:
        print(f" ¬† ‚îî‚îÄ {', '.join(sorted(existing_sources))}")
    
    # X√°c ƒë·ªãnh file c·∫ßn n·∫°p m·ªõi
    target_files = {os.path.basename(p): p for p in pdf_paths}
    new_files = {name: path for name, path in target_files.items() if name not in existing_sources}
    
    if not new_files:
        print(f"\n‚úÖ T·∫•t c·∫£ {len(target_files)} file ƒë√£ c√≥ trong VectorDB!")
        print("üí° D√πng l·ªánh 'reload' ƒë·ªÉ n·∫°p l·∫°i to√†n b·ªô n·∫øu c·∫ßn.\n")
        retriever = vectordb.as_retriever(search_kwargs={"k": 50})
        return vectordb
    
    print(f"\nüì• C·∫ßn n·∫°p {len(new_files)} file m·ªõi:")
    for name in sorted(new_files.keys()):
        print(f" ¬† + {name}")
    print()

    all_new_docs = []
    total_chunks = 0

    # ƒê·ªçc v√† chunk t·ª´ng file PDF m·ªõi
    for filename, path in new_files.items():
        if not os.path.exists(path):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {path}")
            continue

        print(f"üìñ ƒêang ƒë·ªçc: {filename} ...")

        loader = PyMuPDFLoader(path)
        try:
            docs = loader.load()
        except Exception as e:
            print(f"‚ùå L·ªói khi load {filename}: {e}")
            continue

        # G·∫Øn th√¥ng tin ngu·ªìn file
        for i, d in enumerate(docs):
            if d.metadata is None:
                d.metadata = {}
            d.metadata["source"] = filename
            d.metadata["page"] = i + 1

        # Chunk n·ªôi dung
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=300,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        split_docs = splitter.split_documents(docs)

        # G·∫Øn th√™m chunk index
        for i, d in enumerate(split_docs):
            d.metadata["chunk_id"] = i

        print(f" ¬† üîπ T·∫°o {len(split_docs)} ƒëo·∫°n t·ª´ {filename}")
        all_new_docs.extend(split_docs)
        total_chunks += len(split_docs)

    if not all_new_docs:
        print("‚ö†Ô∏è Kh√¥ng c√≥ document m·ªõi n√†o ƒë·ªÉ n·∫°p.")
        retriever = vectordb.as_retriever(search_kwargs={"k": 50})
        return vectordb

    print(f"\nüìö T·ªïng c·ªông: {total_chunks} ƒëo·∫°n n·ªôi dung m·ªõi\n")

    # Th√™m v√†o VectorDB v·ªõi ID duy nh·∫•t
    print("üíæ ƒêang th√™m v√†o VectorDB...")
    ids = []
    for d in all_new_docs:
        src = d.metadata.get("source", "unknown")
        page = d.metadata.get("page", 0)
        chunk = d.metadata.get("chunk_id", 0)
        ids.append(f"{src}_page{page}_chunk{chunk}")

    try:
        # Chia nh·ªè ƒë·ªÉ tr√°nh l·ªói khi batch qu√° l·ªõn
        batch_size = 100
        for i in range(0, len(all_new_docs), batch_size):
            batch_docs = all_new_docs[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            vectordb.add_documents(batch_docs, ids=batch_ids)
            print(f" ¬† ‚úì ƒê√£ th√™m {min(i+batch_size, len(all_new_docs))}/{len(all_new_docs)} documents")
        
        print("‚úÖ ƒê√£ th√™m to√†n b·ªô documents m·ªõi v√†o VectorDB!")
    except Exception as e:
        print(f"‚ùå L·ªói khi th√™m documents: {e}")
        return None

    # C·∫≠p nh·∫≠t retriever
    retriever = vectordb.as_retriever(search_kwargs={"k": 50})

    # Th·ªëng k√™ cu·ªëi c√πng
    try:
        count = vectordb._collection.count()
        final_sources = get_existing_sources()
        print(f"\nüìÇ L∆∞u t·∫°i: {vectordb_path}")
        print(f"üìä VectorDB hi·ªán c√≥:")
        print(f" ¬† ‚Ä¢ T·ªïng documents: {count}")
        print(f" ¬† ‚Ä¢ T·ªïng file: {len(final_sources)}")
        print(f" ¬† ‚Ä¢ Danh s√°ch: {', '.join(sorted(final_sources))}\n")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y th·ªëng k√™: {e}\n")

    return vectordb

# ===================== CLEANING & RETRIEVAL =====================
_URL_RE = re.compile(r"https?://[^\s]+", re.IGNORECASE)

def clean_question_remove_uris(text: str) -> str:
    """L√†m s·∫°ch c√¢u h·ªèi, lo·∫°i b·ªè URL v√† t√™n file PDF"""
    txt = _URL_RE.sub(" ", text or "")
    toks = re.split(r"\s+", txt)
    toks = [t for t in toks if not t.lower().endswith(".pdf")]
    return " ".join(toks).strip()

# Chu·ªói tr·∫£ l·ªùi c·ªë ƒë·ªãnh theo Quy t·∫Øc 3
FIXED_RESPONSE_Q3 = 'N·∫øu b·∫°n mu·ªën bi·∫øt th√™m th√¥ng tin chi ti·∫øt v·ªÅ c√°c c·ª•m, h√£y truy c·∫≠p v√†o website https://iipmap.com/.'

def is_detail_query(text: str) -> bool:
    """Ki·ªÉm tra xem c√¢u h·ªèi c√≥ ph·∫£i l√† c√¢u h·ªèi chi ti·∫øt v·ªÅ khu/c·ª•m c√¥ng nghi·ªáp hay kh√¥ng"""
    text_lower = text.lower()
    keywords = ["n√™u chi ti·∫øt", "chi ti·∫øt v·ªÅ", "th√¥ng tin chi ti·∫øt", "c·ª•m c√¥ng nghi·ªáp", "khu c√¥ng nghi·ªáp"]
    if any(k in text_lower for k in keywords):
        # Tr√°nh nh·∫ßm l·∫´n v·ªõi c√¢u h·ªèi th·ªëng k√™
        if "c√≥ bao nhi√™u" in text_lower or "th·ªëng k√™" in text_lower:
            return False
        return True
    return False

def count_previous_detail_queries(history: List[BaseMessage]) -> int:
    """ƒê·∫øm s·ªë l·∫ßn h·ªèi chi ti·∫øt v·ªÅ KCN/CCN ƒë√£ ƒë∆∞·ª£c tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥ (l·∫ßn ƒë·∫ßu ƒë∆∞·ª£c t√≠nh l√† 0)"""
    count = 0
    # L·∫∑p qua l·ªãch s·ª≠ t·ª´ tin nh·∫Øn c≈© nh·∫•t ƒë·∫øn tin nh·∫Øn g·∫ßn nh·∫•t
    for i in range(len(history)):
        current_message = history[i]
        
        # Ch·ªâ x√©t tin nh·∫Øn HumanMessage v√† tin nh·∫Øn Bot (AIMessage) li·ªÅn k·ªÅ
        if isinstance(current_message, HumanMessage):
            # Ki·ªÉm tra xem tin nh·∫Øn ng∆∞·ªùi d√πng c√≥ ph·∫£i l√† c√¢u h·ªèi chi ti·∫øt kh√¥ng
            is_q = is_detail_query(current_message.content)
            
            # Ki·ªÉm tra c√¢u tr·∫£ l·ªùi li·ªÅn k·ªÅ c·ªßa Bot
            if is_q and i + 1 < len(history) and isinstance(history[i+1], AIMessage):

                bot_response = history[i+1].content
                if FIXED_RESPONSE_Q3 not in bot_response:
                    count += 1

                
    return count

def process_pdf_question(i: Dict[str, Any]) -> str:
    """X·ª≠ l√Ω c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng"""
    global retriever
    
    message = i["message"]
    history: List[BaseMessage] = i.get("history", [])

    # ************************************************
    # B·ªî SUNG LOGIC CHO QUY T·∫ÆC 3 T·∫†I ƒê√ÇY
    # ************************************************
    clean_question = clean_question_remove_uris(message)
    
    if is_detail_query(clean_question):
        count_detail_queries = count_previous_detail_queries(history)

        if count_detail_queries >= 1: # L·∫ßn h·ªèi chi ti·∫øt th·ª© hai tr·ªü ƒëi (ƒë√£ c√≥ 1 l·∫ßn tr·∫£ l·ªùi th√†nh c√¥ng)
            #print(f"üí° Ph√°t hi·ªán h·ªèi chi ti·∫øt l·∫ßn {count_detail_queries + 1}. √Åp d·ª•ng Quy t·∫Øc 3.")
            return FIXED_RESPONSE_Q3
        
        # N·∫øu count_detail_queries == 0, ƒë√¢y l√† l·∫ßn h·ªèi chi ti·∫øt ƒë·∫ßu ti√™n -> Ti·∫øp t·ª•c x·ª≠ l√Ω b√¨nh th∆∞·ªùng.
    # ************************************************
    
    # Ki·ªÉm tra VectorDB
    if not check_vectordb_exists():
        print("‚ö†Ô∏è VectorDB ch∆∞a s·∫µn s√†ng, ƒëang n·∫°p PDF v√†o h·ªá th·ªëng...")
        result = ingest_pdf()
        if result is None:
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi n·∫°p t√†i li·ªáu PDF. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file."

    
    try:
        # T√¨m ki·∫øm trong VectorDB
        hits = retriever.invoke(clean_question)
        
        if not hits:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu PDF."

        # X√¢y d·ª±ng context t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
        context = build_context_from_hits(hits, max_chars=6000)
        
        # T·∫°o messages
        messages = [SystemMessage(content=PDF_READER_SYS)]

        if history:
            messages.extend(history[-10:])  # Ch·ªâ l·∫•y 10 tin nh·∫Øn g·∫ßn nh·∫•t

        user_message = f"""C√¢u h·ªèi: {clean_question}

N·ªôi dung li√™n quan t·ª´ t√†i li·ªáu PDF:
{context}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n c√°c n·ªôi dung tr√™n."""
        
        messages.append(HumanMessage(content=user_message))
        
        # G·ªçi LLM
        response = llm.invoke(messages).content
        
        # ************************************************
        # PH·∫¢I L∆ØU TR·ªÆ L·∫†I C√ÇU TR·∫¢ L·ªúI ƒê·ªÇ C√ì TH·ªÇ ƒê·∫æM ƒê√öNG
        # Trong c·∫•u tr√∫c LangChain RunnableWithMessageHistory, vi·ªác l∆∞u tr·ªØ di·ªÖn ra sau h√†m n√†y.
        # Logic ƒë·∫øm ·ªü tr√™n l√† ƒë·ªß ƒë·ªÉ ch·∫∑n.
        # ************************************************
        
        return response

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return f"Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"

# ===================== MAIN CHATBOT =====================
pdf_chain = RunnableLambda(process_pdf_question)
store: Dict[str, ChatMessageHistory] = {}

def get_history(session_id: str):
    """L·∫•y ho·∫∑c t·∫°o l·ªãch s·ª≠ chat cho session"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

chatbot = RunnableWithMessageHistory(
    pdf_chain,
    get_history,
    input_messages_key="message",
    history_messages_key="history"
)

def print_help():
    """In h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"""
    print("\n" + "="*60)
    print("üìö C√ÅC L·ªÜNH C√ì S·∫¥N:")
    print("="*60)
    print(" - exit / quit ¬†: Tho√°t ch∆∞∆°ng tr√¨nh")
    print(" - clear ¬† ¬† ¬† ¬†: X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i")
    print(" - sync ¬† ¬† ¬† ¬† : ƒê·ªìng b·ªô file m·ªõi t·ª´ folder v√†o VectorDB")
    print(" - reload ¬† ¬† ¬† : X√≥a to√†n b·ªô v√† n·∫°p l·∫°i (force reload)")
    print(" - status ¬† ¬† ¬† : Ki·ªÉm tra tr·∫°ng th√°i VectorDB")
    print(" - help ¬† ¬† ¬† ¬† : Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n√†y")
    print("="*60 + "\n")

def handle_command(command: str, session: str) -> bool:
    """X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát"""
    global vectordb, retriever
    cmd = command.lower().strip()

    if cmd in {"exit", "quit"}:
        print("\nüëã T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i!")
        return False
    
    elif cmd == "clear":
        if session in store:
            store[session].clear()
            print("üßπ ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.\n")
        return True
    
    elif cmd == "reload":
        print("üîÑ ƒêang x√≥a v√† n·∫°p l·∫°i to√†n b·ªô PDF...")
        ingest_pdf(force_reload=True)
        return True
    
    elif cmd == "status":
        stats = get_vectordb_stats()
        print("\n" + "="*60)
        print("üìä TR·∫†NG TH√ÅI VECTORDB")
        print("="*60)
        if stats["exists"]:
            print(f"‚úÖ Tr·∫°ng th√°i: S·∫µn s√†ng")
            print(f"üìä T·ªïng documents: {stats['total_documents']}")
            print(f"üìÇ ƒê∆∞·ªùng d·∫´n: {stats['path']}")
            print(f"üìò C√°c file ƒë√£ n·∫°p:")
            for src in stats.get('sources', []):
                print(f" ¬† - {src}")
        else:
            print("‚ùå Tr·∫°ng th√°i: Ch∆∞a s·∫µn s√†ng")
            print("üí° H√£y ƒë·ª£i h·ªá th·ªëng n·∫°p PDF ho·∫∑c g√µ 'reload'")
        print("="*60 + "\n")
        return True
    
    elif cmd == "help":
        print_help()
        return True
    
    else:
        return True

# ===================== CLI =====================
if __name__ == "__main__":
    session = "pdf_reader_session"

    print("\n" + "="*60)
    print("ü§ñ CHATBOT C·ªîNG VI·ªÜC L√ÄM VI·ªÜT NAM")
    print("="*60)
    print(f"üìÅ Folder t√†i li·ªáu: {PDF_FOLDER}")
    print(f"üìö T√¨m th·∫•y {len(PDF_PATHS)} file PDF:")
    
    if PDF_PATHS:
        for idx, p in enumerate(PDF_PATHS, 1):
            status = "‚úÖ" if os.path.exists(p) else "‚ùå"
            print(f" ¬† {idx}. {status} {os.path.basename(p)}")
    else:
        print(" ¬† ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o trong folder!")
    
    print(f"\nüìÇ VectorDB: {VECTORDB_PATH}")
    print("üîç T√¥i h·ªó tr·ª£: Lu·∫≠t Lao ƒë·ªông & Lu·∫≠t D√¢n s·ª± Vi·ªát Nam")
    print_help()

    # Kh·ªüi t·∫°o VectorDB
    if not PDF_PATHS:
        print("‚ùå Kh√¥ng c√≥ file PDF n√†o ƒë·ªÉ x·ª≠ l√Ω. Vui l√≤ng ki·ªÉm tra l·∫°i folder.")
        exit(1)
    
    if check_vectordb_exists():
        stats = get_vectordb_stats()
        print(f"‚úÖ VectorDB s·∫µn s√†ng v·ªõi {stats['total_documents']} documents")
        print(f"üìö ƒê√£ n·∫°p: {', '.join(stats.get('sources', []))}\n")
    else:
        print("üì• ƒêang n·∫°p PDF l·∫ßn ƒë·∫ßu ti√™n...")
        result = ingest_pdf()
        if result is None:
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o VectorDB. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file PDF.")
            exit(1)

    print("üí¨ S·∫µn s√†ng tr·∫£ l·ªùi c√¢u h·ªèi! (G√µ 'help' ƒë·ªÉ xem h∆∞·ªõng d·∫´n)\n")

    # Main loop
    while True:
        try:
            message = input("üë§ B·∫°n: ").strip()
            
            if not message:
                continue
            
            # X·ª≠ l√Ω l·ªánh
            if not handle_command(message, session):
                break
            
            # B·ªè qua n·∫øu l√† l·ªánh
            if message.lower() in ["clear", "reload", "status", "help"]:
                continue
            
            # X·ª≠ l√Ω c√¢u h·ªèi th∆∞·ªùng
            print("üîé ƒêang t√¨m ki·∫øm trong t√†i li·ªáu...")
            response = chatbot.invoke(
                {"message": message},
                config={"configurable": {"session_id": session}}
            )
            print(f"\nü§ñ Bot: {response}\n")
            print("-" * 60 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nüëã T·∫°m bi·ªát!")
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói: {e}\n")